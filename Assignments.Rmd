---
title: "Lab 1 Topic 1 Block 2 Machine Learning"
author: "Hugo Knape & Zahra Jalil Pour & Niklas Larsson"
date: "11/12/2020"
output:
  pdf_document: default
  word_document: default
latex_engine: xelatex
---

# State of contribution

### Assignment1: Niklas Larsson
### Assignment2: Hugo Knape
### Assignment3: Zahra Jalilpour

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, warning = FALSE, include = FALSE}
library(kknn)
library(ggplot2)
```


# Assigment 1: Ensembled methods

## Task A

### Generate test data:
```{r}

library(randomForest)
set.seed(1234)

x1<-runif(1000)
x2<-runif(1000)
tedataA<-cbind(x1,x2)
y<-as.numeric(x1<x2)
telabelsA<-as.factor(y)
plot(x1,x2,col=(y+1))


forestsize = c(1,10,100)
results = matrix(0,100,3)

for (i in 1:3){
  ntree = forestsize[i]
  
  for(j in 1:100){
    x1<-runif(100)
    x2<-runif(100)
    trdata<-cbind(x1,x2)
    y<-as.numeric(x1<x2)
    trlabels<-as.factor(y)
    
    fitA = randomForest(trdata, trlabels, ntree=ntree, nodesize = 25, keep.forest = TRUE)
    predA = predict(fitA, tedataA)
    results[j,i] = sum(as.numeric(predA != telabelsA))/length(telabelsA)
  }
}

resdataA = matrix(0,3,2)
dimnames(resdataA) = list(c("[1]","[10]","[100]"),c("Mean","Variance"))
for(i in 1:3){
  resdataA[i,1] = mean(results[,i])
  resdataA[i,2] = var(results[,i])
}
```


## Task B

### New data generation
```{r}

set.seed(1234)

x1<-runif(1000)
x2<-runif(1000)
tedataB<-cbind(x1,x2)
y<-as.numeric(x1<0.5)
telabelsB<-as.factor(y)
plot(x1,x2,col=(y+1))

forestsize = c(1,10,100)
results = matrix(0,100,3)

for (i in 1:3){
  ntree = forestsize[i]
  
  for(j in 1:100){
    x1<-runif(100)
    x2<-runif(100)
    trdata<-cbind(x1,x2)
    y<-as.numeric(x1<0.5)
    trlabels<-as.factor(y)
    
    fitB = randomForest(trdata, trlabels, ntree=ntree, nodesize = 25, keep.forest = TRUE)
    predB = predict(fitB, tedataB)
    results[j,i] = sum(as.numeric(predB != telabelsB))/length(telabelsB)
  }
}

resdataB = matrix(0,3,2)
dimnames(resdataB) = list(c("[1]","[10]","[100]"),c("Mean","Variance"))
for(i in 1:3){
  resdataB[i,1] = mean(results[,i])
  resdataB[i,2] = var(results[,i])
}
```




## Task C

### New data generation
```{r}

set.seed(1234)

x1<-runif(1000)
x2<-runif(1000)
tedataC<-cbind(x1,x2)
y<-as.numeric((x1<0.5 & x2<0.5) | (x1>0.5 & x2>0.5))
telabelsC<-as.factor(y)
plot(x1,x2,col=(y+1))

forestsize = c(1,10,100)
results = matrix(0,100,3)

for (i in 1:3){
  ntree = forestsize[i]
  
  for(j in 1:100){
    x1<-runif(100)
    x2<-runif(100)
    trdata<-cbind(x1,x2)
    y<-as.numeric( (x1<0.5 & x2<0.5) | (x1>0.5 & x2>0.5) )
    trlabels<-as.factor(y)
    
    fitC = randomForest(trdata, trlabels, ntree=ntree, nodesize = 12, keep.forest = TRUE)
    predC = predict(fitC, tedataC)
    results[j,i] = sum(as.numeric(predC != telabelsC))/length(telabelsC)
  }
}

resdataC = matrix(0,3,2)
dimnames(resdataC) = list(c("[1]","[10]","[100]"),c("Mean","Variance"))
for(i in 1:3){
  resdataC[i,1] = mean(results[,i])
  resdataC[i,2] = var(results[,i])
}
```



## Task D
### Question A
Question: What happens with the mean and variance of the error rate when the number of trees in the random forest grows ?

As seen in all three cases the mean error rate decreases with an increasing number of trees in the forest. The same applies
for the variance of the error except for case 1 where the variance increased just slightly between the 10- and 100-trees models.
```{r}

print("Task A results:", quote = FALSE)
print(resdataA)

print("Task B results:", quote = FALSE)
print(resdataB)

print("Task B results:", quote = FALSE)
print(resdataC)
```


### Question B

Due to the node size parameter. Using a smaller minimum node size allows the tree to grow more, in other words the model can divide
the data into more specific sections where it can label data. Using smaller node size will need more computation power and could 
potentially overfit to the data while too large size does the opposite.

### Question C

The variance is a measure which tells how much the resulting misclassification are deviating from the mean error.
Having a lower variance gives a higher certainty regarding the mean error rate.


# Assignment 2: Mixture models

```{r}
EM_algo <- function(k_num){
set.seed(1234567890)
max_it <- 100 # max number of EM iterations
min_change <- 0.1 # min change in log likelihood between two consecutive EM iterations
N=1000 # number of training points
D=10 # number of dimensions
x <- matrix(nrow=N, ncol=D) # training data

true_pi <- vector(length = 3) # true mixing coefficients
true_mu <- matrix(nrow=3, ncol=D) # true conditional distributions
true_pi=c(1/3, 1/3, 1/3)
true_mu[1,]=c(0.5,0.6,0.4,0.7,0.3,0.8,0.2,0.9,0.1,1)
true_mu[2,]=c(0.5,0.4,0.6,0.3,0.7,0.2,0.8,0.1,0.9,0)
true_mu[3,]=c(0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5)

# Producing the training data
for(n in 1:N) {
  k <- sample(1:3,1,prob=true_pi)
  for(d in 1:D) {
    x[n,d] <- rbinom(1,1,true_mu[k,d])
  }
}

K <- k_num# number of guessed components
z <- matrix(nrow=N, ncol=K) # fractional component assignments
pi <- vector(length = K) # mixing coefficients
mu <- matrix(nrow=K, ncol=D) # conditional distributions
llik <- vector(length = max_it) # log likelihood of the EM iterations

# Random initialization of the paramters
pi <- runif(K,0.49,0.51)
pi <- pi / sum(pi)
for(k in 1:K) {
  mu[k,] <- runif(D,0.49,0.51)
}

#pi
#mu

for(it in 1:max_it) {
  #plot(mu[1,], type="o", col="blue", ylim=c(0,1))
  #points(mu[2,], type="o", col="red")
  #points(mu[3,], type="o", col="green")
  #points(mu[4,], type="o", col="yellow")
  Sys.sleep(0.5)
  
  # E-step: Computation of the fractional component assignments
  for (j in 1:k) {
    for (i in 1:n) {
      z[i,j] <- pi[j]*prod((mu[j,]^x[i,])*((1-mu[j,])^(1-x[i,])))
    }
  }
  for(j in 1:nrow(z)) {
    z[j,] <- z[j,]/sum(z[j,])
  }
  
  #Log likelihood computation.
  for(i in 1:n ){
    for(j in 1:k){
      llik[it] <- llik[it] + z[i,j] * (log(pi[j]) + sum(x[i,] * log(mu[j,]) + (1- x[i,])*log(1- mu[j,]))) 
    }
  }  
  
  cat("iteration: ", it, "log likelihood: ", llik[it], "\n")
  flush.console() 
  # Stop if the lok likelihood has not changed significantly
  if(it > 1 ){
    if(( abs(llik[it]- llik[it-1])  < min_change)){
      break
    }
  }   
  #M-step: ML parameter estimation from the data and fractional component assignments
  row_sum_z <- c(rep(NA, ncol(z)))
  for (i in 1:ncol(z)) {
    row_sum_z[i] <- sum(z[,i])
  }
  pi <- row_sum_z/N
  mu <- t(z) %*% x /row_sum_z
  
}
return(list(pi = pi))
}
```
For the E-step for mixtures of multivariate Bernoulli distributions we compute:

$$
p\left(z_{n k} \mid \boldsymbol{x}_{n}, \boldsymbol{\mu}, \boldsymbol{\pi}\right)=\frac{p\left(\boldsymbol{z}_{n k}, \boldsymbol{x}_{n} \mid \boldsymbol{\mu}, \boldsymbol{\pi}\right)}{\sum_{k} p\left(z_{n k}, \boldsymbol{x}_{n} \mid \boldsymbol{\mu}, \boldsymbol{\pi}\right)}=\frac{\pi_{k} p\left(\boldsymbol{x}_{n} \mid \boldsymbol{\mu}_{k}\right)}{\sum_{k} \pi_{k} p\left(\boldsymbol{x}_{n} \mid \boldsymbol{\mu}_{k}\right)}
$$
for all n and k 

where

$$
p(\boldsymbol{x}_n \mid \boldsymbol{\mu}_k) = \prod_{i} \mu_{k i}^{x_{i}}\left(1-\mu_{k i}\right)^{\left(1-x_{i}\right)}
$$
The code is:

```{r, eval=FALSE}
  # E-step: Computation of the fractional component assignments
  for (j in 1:k) {
    for (i in 1:n) {
      z[i,j] <- pi[j]*prod((mu[j,]^x[i,])*((1-mu[j,])^(1-x[i,])))
    }
  }
  for(j in 1:nrow(z)) {
    z[j,] <- z[j,]/sum(z[j,])
  }
```

For computing the Log likelihood we use:

$$
\sum_{n} \sum_{k} p\left(z_{n k} \mid \boldsymbol{x}_{n}, \boldsymbol{\mu}, \boldsymbol{\pi}\right)\left[\log \pi_{k}+\sum_{i}\left[x_{n i} \log \mu_{k i}+\left(1-x_{n i}\right) \log \left(1-\mu_{k i}\right)\right]\right]
$$

The code is:
```{r, eval=FALSE}
  #Log likelihood computation.
  for(i in 1:n ){
    for(j in 1:k){
      llik[it] <- llik[it] + z[i,j] * (log(pi[j]) + sum(x[i,] * log(mu[j,]) + (1- x[i,])*log(1- mu[j,]))) 
    }
  } 
 
  
```





ML parameter estimation from the data and fractional component assignments we use:

$$
\begin{aligned}
\pi_{k}^{M L} &=\frac{\sum_{n} p\left(z_{n k} \mid \boldsymbol{x}_{n}, \boldsymbol{\mu}, \boldsymbol{\pi}\right)}{N} \\
\mu_{k i}^{M L} &=\frac{\sum_{n} x_{n i} p\left(z_{n k} \mid \boldsymbol{x}_{n}, \boldsymbol{\mu}, \boldsymbol{\pi}\right)}{\sum_{n} p\left(z_{n k} \mid \boldsymbol{x}_{n}, \boldsymbol{\mu}, \boldsymbol{\pi}\right)}
\end{aligned}
$$
The code is:

```{r, eval=FALSE}
  #M-step: ML parameter estimation from the data and fractional component assignments
  row_sum_z <- c(rep(NA, ncol(z)))
  for (i in 1:ncol(z)) {
    row_sum_z[i] <- sum(z[,i])
  }
```


```{r}
EM_algo(2)
```
When K is equal to 2 the EM-algorithm stops after 16 iterations and the pi values are very close to each other. In this case we miss one true pi, which maybe can be seen as under fitting.

```{r}
EM_algo(3)
```
When K is equal to 3 the EM-algorithm stops after 62 iterations and the pi values are pretty close to each other.

```{r}
EM_algo(4)
```

When K is equal to 4 the EM-algorithm stops after 16 iterations and the pi values are pretty far from each other. In this task we have one extra pi, which maybe can be seen as over fitting.

# Assignment3 . High Dimentional Methods

#### 1
Diviving data to train and test(70/30) and performing NSC classification.
```{r, warning=FALSE, message=FALSE,echo=F,results = 'hide'}
setwd("D:/Linkoping university/first semester/Machine Learning/lab/block2")

new_df <- read.csv("geneexp.csv")

df = new_df[,-1]  # the first and the last are factor
set.seed(12345)
m <- nrow(df)
# divide data to train and test(70:30) 
id <- sample(1:m, floor(m*0.7))
train <- df[id,]
test <- df[-id,]

### train ###
rownames(train) = 1:nrow(train)
x = t(train[, -ncol(train)])
y = train[[ncol(train)]]
mydata <- list(x=x, y=as.factor(y),geneid=as.character(1:nrow(x)),genenames=rownames(x))

### test ###
rownames(test) = 1:nrow(test)
x_test = t(test[, -ncol(test)])
y_test = test[[ncol(test)]]
test_mydata <- list(x=x_test, y=as.factor(y_test),geneid=as.character(1:nrow(x_test)),genenames=rownames(x_test))

```
Then run to train a nearest shrunken centroid classifier.(NSC)
For 30 different values of the threshold the number of nonzero genes and the number of misclassifications on the training set will be  listed


```{r ,message=FALSE,warning=FALSE,echo=F, results= 'hide'}
library(pamr)
model = pamr.train(mydata)

# choice the threshold by cv
cvmodel = pamr.cv(model,mydata)
```

```{r}
model
```
```{r}
cvmodel


```
The output of this function looks very similar to the model. The numbers in the errors column are now the summed errors of all 10 cross validation steps. The CV error usually is bigger than the training error.

Missclassification Error Plot.
In both figures, the x-axis represents different values of threshold (corresponding to different numbers of nonzero genes as shown on top of each figure and
the y-axis shows the number of misclassifications. The upper figure describes the whole
dataset, the lower one describes each class individually
```{r,fig.width=12, fig.height=8}
pamr.plotcv(cvmodel)

```
Minimum threshold:
```{r message=FALSE, warning=FALSE,echo=FALSE}
min_Error <- which.min(cvmodel$error)
best_Th <- cvmodel$threshold[min_Error]
paste("Best threshold equals: ", best_Th)
numberofgen <- model$nonzero[min_Error]
paste("Number of features: ", numberofgen)
```
The value of 7.2 of the threshold has the lowest error of cv.model.

Centroid plot(threshold = 7.2):

The function pamr.plotcen() plots the shrunken class centroids for each class.NSC classification shrink each of the class centroid toward the overall centroid for all classes by threshold.This shrinkage consists of moving the centroid toward zero by threshold, setting it equal to zero if it hits zero.
Here the threshold is 7.2, hence the class centroid would be shrink be minus by the threshold. It is the reason that we see positive and negative values in the centroid plot.
```{r}
pamr.plotcen(model, mydata, threshold = best_Th)


```
Number of genes selected by this model:
```{r message=FALSE, warning=FALSE,echo=FALSE}
a<-pamr.listgenes(model, mydata, threshold = best_Th, genenames=FALSE)
nrow(a)
```
```{r}
paste("Number of genes selected by this model : ", nrow(a))
```
#### task_2
names of the 2 most contributing genes 
```{r message=FALSE, warning=FALSE,echo=FALSE}
a<-(pamr.listgenes(model, mydata, threshold = best_Th))
top_2 = a[1:2,1]
k <- as.numeric(top_2)
mydata$genenames[k]
```
```{r}
paste("Names of the two most contributing genes: ",mydata$genenames[k] )
```
The name of two most contributing genes are, " CD74" and "HLA-DRA".And both of them are considered as marker genes.

Confusion Matrix.
Here we see 73 samples belong to class CD19, 65 sample is classified correctly and 8 are missclassified
as CD4.  70 samples belong to CD4. 65 sample is classified correctly and 5 are missclassified as CD8
67 samples belong to CD8, 59 is classified correctly and 8 is missclassified as CD4. This makes an 
overall error rate 0.1%.
```{r}
pamr.confusion(cvmodel,  threshold = best_Th)
```
##### reporting test error.
```{r message=FALSE, warning=FALSE,echo=FALSE}
pam.diagnosis <- pamr.predict(model, x_test, threshold = best_Th)
new_matrix <- table(y_test, pam.diagnosis)
paste("Confusion Matrix for test data :")
new_matrix
test_ME <- 1-(sum(diag(new_matrix)))/sum(new_matrix)
cat(paste("ME rate for test data :  ", test_ME))
```
It seems that 27 samples are classified correctly to CD19, 30 samples belong to CD4, in which 4 samples are miss classified. 33 samples belong to CD8,6 samples from them are miss qualified . 

#### task_3
##### Elastic net with multinomial response :
```{r }
library(glmnet)
set.seed(12345)
x_e <- as.matrix(train[, -ncol(train)])
y_e <- as.matrix(train[[ncol(train)]])
# fit the elastic net
elastic <- cv.glmnet(x_e, y_e, alpha = 0.5, family = "multinomial")
testx=as.matrix((test[ , -ncol(test)]))
predenet=predict(elastic, newx = testx, s = elastic$lambda.min, type = "class")
cm_elastic_net <- table(y_test, predenet)
cm_elastic_net
test_error_elasic_net <- 1-sum(diag(cm_elastic_net))/sum(cm_elastic_net)
coef1 <-coef(elastic, s="lambda.min")
num_feature<-length(coef1$CD8@x)+length(coef1$CD19@x)+length(coef1$CD4@x)
cat(paste("ME rate for test data :  ", test_error_elasic_net))
paste("Number of feature :",num_feature )

```
##### SVM:
```{r}
library("kernlab")
set.seed(12345)
train.x <-as.matrix(train[, -ncol(train)])
train.y <- as.matrix(train[[ncol(train)]])
svm.trained <- ksvm(train.x, train.y , type = "C-svc", kernel='vanilladot')
svm_pred <- predict(svm.trained, testx)
cm_svm <- table(y_test, svm_pred)
cm_svm
test_error_svm = 1-sum(diag(cm_svm))/sum(cm_svm)
paste("number of feature:", (ncol(df)-1))
cat(paste("ME rate for test data :  ", test_error_svm))
```
```{r message=FALSE, warning=FALSE,echo=FALSE}
final_result <- cbind(numberofgen, num_feature, (ncol(df)-1))
feature_count <- cbind(test_ME, test_error_elasic_net,test_error_svm)
final_matrix <- rbind(final_result,feature_count)
colnames(final_matrix) <- c("Nearest Shrunken Centroid Model", "Elastic_Net Model","SVM Model")
rownames(final_matrix) <- c("Number of Features", "Error rate")
knitr::kable(final_matrix, caption = "Comparsion of Three models")
```
Analysis:
By comparison three different models, we can see the SVM has the lowest error rate compare two other models, but the this model select 2085 features for classifying three different classes.
The error rate in Elastic net model is lower than NSC.but number of features in three classes are more than NSC. As the difference between error rate in NSC and Elastic net could be ignored, by considering number of features, NSC could be a good model for classification.
#### task4: Benjamin Hochberg
```{r , echo= FALSE }
df = new_df[,-1]
y_1 <- ifelse(df$CellType=="CD4",1,0)
y_2 <- ifelse(df$CellType=="CD8",1,0)
y_3 <- ifelse(df$CellType=="CD19",1,0)
p1 <- sapply(1:(ncol(df)-1), function(x) t.test(df[,x] ~ y_1, data = df)$p.val)
p2 <- sapply(1:(ncol(df)-1), function(x) t.test(df[,x] ~ y_2, data = df)$p.val)
p3 <- sapply(1:(ncol(df)-1), function(x) t.test(df[,x] ~ y_3, data = df)$p.val)

pvall_1 <- as.data.frame(p1)
pvall_2 <- as.data.frame(p2)
pvall_3 <- as.data.frame(p3)
no_CD4 <-length(pvall_1$p1[which(p1 < 0.05)])
paste("number of features in CD4: ", no_CD4)
no_CD8 <-length(pvall_2$p2[which(p2 < 0.05)])
paste("number of features in CD8: ", no_CD8)
no_CD19 <-length(pvall_3$p3[which(p3 < 0.05)])
paste("number of features in CD19: ", no_CD19)
feature_CD4<-as.data.frame(pvall_1$p1[which(p1 < 0.05)])
names(feature_CD4)[1]<-"feature_sel"
feature_CD8<-as.data.frame(pvall_2$p2[which(p2 < 0.05)])
names(feature_CD8)[1]<-"feature_sel"
feature_CD19<-as.data.frame(pvall_3$p3[which(p3 < 0.05)])
names(feature_CD19)[1]<-"feature_sel"
x1 <- sort(as.numeric(p1))
x2 <- sort(as.numeric(p2))
x3 <- sort(as.numeric(p3))
y1<- sort(as.numeric(feature_CD4$feature_sel))
y2<- sort(as.numeric(feature_CD8$feature_sel))
y3 <- sort(as.numeric(feature_CD19$feature_sel))
library(ggplot2)
ggplot()+
  geom_density(aes(x1),col="red")+
  geom_density(aes(y1) , col="blue")+
  ggtitle("p_values and rejected area for CD4")
ggplot()+
  geom_density(aes(x2),col="red")+
  geom_density(aes(y2) , col="blue")+
  ggtitle("p_values and rejected area for CD8")
ggplot()+
  geom_density(aes(x3),col="red")+
  geom_density(aes(y3) , col="blue")+
  ggtitle("p_values and rejected area for CD19")



```
The Benjamin_Hochberg method is implemented. By using t.test, varoius p_value are calculated for three different classes.The number of features that are not related to three classes are more than three other models.


# Appendix:

```{r ref.label = knitr::all_labels(), echo=TRUE, eval=FALSE}
```