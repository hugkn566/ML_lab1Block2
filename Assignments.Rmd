---
title: "Lab 1 Topic 1 Block 2 Machine Learning"
author: "Hugo Knape & Zahra Jalil Pour & Niklas Larsson"
date: "11/12/2020"
output:
  pdf_document: default
  word_document: default
latex_engine: xelatex
---

# State of contribution 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, warning = FALSE, include = FALSE}
library(kknn)
library(ggplot2)
```


# Assigment 1: Ensembled methods

## Task A

### Generate test data:
```{r}

library(randomForest)
set.seed(1234)

x1<-runif(1000)
x2<-runif(1000)
tedataA<-cbind(x1,x2)
y<-as.numeric(x1<x2)
telabelsA<-as.factor(y)
plot(x1,x2,col=(y+1))


forestsize = c(1,10,100)
results = matrix(0,100,3)

for (i in 1:3){
  ntree = forestsize[i]
  
  for(j in 1:100){
    x1<-runif(100)
    x2<-runif(100)
    trdata<-cbind(x1,x2)
    y<-as.numeric(x1<x2)
    trlabels<-as.factor(y)
    
    fitA = randomForest(trdata, trlabels, ntree=ntree, nodesize = 25, keep.forest = TRUE)
    predA = predict(fitA, tedataA)
    results[j,i] = sum(as.numeric(predA != telabelsA))/length(telabelsA)
  }
}

resdataA = matrix(0,3,2)
dimnames(resdataA) = list(c("[1]","[10]","[100]"),c("Mean","Variance"))
for(i in 1:3){
  resdataA[i,1] = mean(results[,i])
  resdataA[i,2] = var(results[,i])
}
```


## Task B

### New data generation
```{r}

set.seed(1234)

x1<-runif(1000)
x2<-runif(1000)
tedataB<-cbind(x1,x2)
y<-as.numeric(x1<0.5)
telabelsB<-as.factor(y)
plot(x1,x2,col=(y+1))

forestsize = c(1,10,100)
results = matrix(0,100,3)

for (i in 1:3){
  ntree = forestsize[i]
  
  for(j in 1:100){
    x1<-runif(100)
    x2<-runif(100)
    trdata<-cbind(x1,x2)
    y<-as.numeric(x1<0.5)
    trlabels<-as.factor(y)
    
    fitB = randomForest(trdata, trlabels, ntree=ntree, nodesize = 25, keep.forest = TRUE)
    predB = predict(fitB, tedataB)
    results[j,i] = sum(as.numeric(predB != telabelsB))/length(telabelsB)
  }
}

resdataB = matrix(0,3,2)
dimnames(resdataB) = list(c("[1]","[10]","[100]"),c("Mean","Variance"))
for(i in 1:3){
  resdataB[i,1] = mean(results[,i])
  resdataB[i,2] = var(results[,i])
}
```




## Task C

### New data generation
```{r}

set.seed(1234)

x1<-runif(1000)
x2<-runif(1000)
tedataC<-cbind(x1,x2)
y<-as.numeric((x1<0.5 & x2<0.5) | (x1>0.5 & x2>0.5))
telabelsC<-as.factor(y)
plot(x1,x2,col=(y+1))

forestsize = c(1,10,100)
results = matrix(0,100,3)

for (i in 1:3){
  ntree = forestsize[i]
  
  for(j in 1:100){
    x1<-runif(100)
    x2<-runif(100)
    trdata<-cbind(x1,x2)
    y<-as.numeric( (x1<0.5 & x2<0.5) | (x1>0.5 & x2>0.5) )
    trlabels<-as.factor(y)
    
    fitC = randomForest(trdata, trlabels, ntree=ntree, nodesize = 12, keep.forest = TRUE)
    predC = predict(fitC, tedataC)
    results[j,i] = sum(as.numeric(predC != telabelsC))/length(telabelsC)
  }
}

resdataC = matrix(0,3,2)
dimnames(resdataC) = list(c("[1]","[10]","[100]"),c("Mean","Variance"))
for(i in 1:3){
  resdataC[i,1] = mean(results[,i])
  resdataC[i,2] = var(results[,i])
}
```



## Task D
### Question A
Question: What happens with the mean and variance of the error rate when the number of trees in the random forest grows ?

As seen in all three cases the mean error rate decreases with an increasing number of trees in the forest. The same applies
for the variance of the error except for case 1 where the variance increased just slightly between the 10- and 100-trees models.
```{r}

print("Task A results:", quote = FALSE)
print(resdataA)

print("Task B results:", quote = FALSE)
print(resdataB)

print("Task B results:", quote = FALSE)
print(resdataC)
```


### Question B

Due to the node size parameter. Using a smaller minimum node size allows the tree to grow more, in other words the model can divide
the data into more specific sections where it can label data. Using smaller node size will need more computation power and could 
potentially overfit to the data while too large size does the opposite.

### Question C

The variance is a measure which tells how much the resulting misclassification are deviating from the mean error.
Having a lower variance gives a higher certainty regarding the mean error rate.


# Assignment 2: Mixture models

```{r}
EM_algo <- function(k_num){
set.seed(1234567890)
max_it <- 100 # max number of EM iterations
min_change <- 0.1 # min change in log likelihood between two consecutive EM iterations
N=1000 # number of training points
D=10 # number of dimensions
x <- matrix(nrow=N, ncol=D) # training data

true_pi <- vector(length = 3) # true mixing coefficients
true_mu <- matrix(nrow=3, ncol=D) # true conditional distributions
true_pi=c(1/3, 1/3, 1/3)
true_mu[1,]=c(0.5,0.6,0.4,0.7,0.3,0.8,0.2,0.9,0.1,1)
true_mu[2,]=c(0.5,0.4,0.6,0.3,0.7,0.2,0.8,0.1,0.9,0)
true_mu[3,]=c(0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5)

# Producing the training data
for(n in 1:N) {
  k <- sample(1:3,1,prob=true_pi)
  for(d in 1:D) {
    x[n,d] <- rbinom(1,1,true_mu[k,d])
  }
}

K <- k_num# number of guessed components
z <- matrix(nrow=N, ncol=K) # fractional component assignments
pi <- vector(length = K) # mixing coefficients
mu <- matrix(nrow=K, ncol=D) # conditional distributions
llik <- vector(length = max_it) # log likelihood of the EM iterations

# Random initialization of the paramters
pi <- runif(K,0.49,0.51)
pi <- pi / sum(pi)
for(k in 1:K) {
  mu[k,] <- runif(D,0.49,0.51)
}

#pi
#mu

for(it in 1:max_it) {
  #plot(mu[1,], type="o", col="blue", ylim=c(0,1))
  #points(mu[2,], type="o", col="red")
  #points(mu[3,], type="o", col="green")
  #points(mu[4,], type="o", col="yellow")
  Sys.sleep(0.5)
  
  # E-step: Computation of the fractional component assignments
  for (j in 1:k) {
    for (i in 1:n) {
      z[i,j] <- pi[j]*prod((mu[j,]^x[i,])*((1-mu[j,])^(1-x[i,])))
    }
  }
  for(j in 1:nrow(z)) {
    z[j,] <- z[j,]/sum(z[j,])
  }
  
  #Log likelihood computation.
  for(i in 1:n ){
    for(j in 1:k){
      llik[it] <- llik[it] + z[i,j] * (log(pi[j]) + sum(x[i,] * log(mu[j,]) + (1- x[i,])*log(1- mu[j,]))) 
    }
  }  
  
  cat("iteration: ", it, "log likelihood: ", llik[it], "\n")
  flush.console() 
  # Stop if the lok likelihood has not changed significantly
  if(it > 1 ){
    if(( abs(llik[it]- llik[it-1])  < min_change)){
      break
    }
  }   
  #M-step: ML parameter estimation from the data and fractional component assignments
  row_sum_z <- c(rep(NA, ncol(z)))
  for (i in 1:ncol(z)) {
    row_sum_z[i] <- sum(z[,i])
  }
  pi <- row_sum_z/N
  mu <- t(z) %*% x /row_sum_z
  
}
return(list(pi = pi))
}
```
For the E-step for mixtures of multivariate Bernoulli distributions we compute:

$$
p\left(z_{n k} \mid \boldsymbol{x}_{n}, \boldsymbol{\mu}, \boldsymbol{\pi}\right)=\frac{p\left(\boldsymbol{z}_{n k}, \boldsymbol{x}_{n} \mid \boldsymbol{\mu}, \boldsymbol{\pi}\right)}{\sum_{k} p\left(z_{n k}, \boldsymbol{x}_{n} \mid \boldsymbol{\mu}, \boldsymbol{\pi}\right)}=\frac{\pi_{k} p\left(\boldsymbol{x}_{n} \mid \boldsymbol{\mu}_{k}\right)}{\sum_{k} \pi_{k} p\left(\boldsymbol{x}_{n} \mid \boldsymbol{\mu}_{k}\right)}
$$
for all n and k 

where

$$
p(\boldsymbol{x}_n \mid \boldsymbol{\mu}_k) = \prod_{i} \mu_{k i}^{x_{i}}\left(1-\mu_{k i}\right)^{\left(1-x_{i}\right)}
$$
The code is:

```{r, eval=FALSE}
  # E-step: Computation of the fractional component assignments
  for (j in 1:k) {
    for (i in 1:n) {
      z[i,j] <- pi[j]*prod((mu[j,]^x[i,])*((1-mu[j,])^(1-x[i,])))
    }
  }
  for(j in 1:nrow(z)) {
    z[j,] <- z[j,]/sum(z[j,])
  }
```

For computing the Log likelihood we use:

$$
\sum_{n} \sum_{k} p\left(z_{n k} \mid \boldsymbol{x}_{n}, \boldsymbol{\mu}, \boldsymbol{\pi}\right)\left[\log \pi_{k}+\sum_{i}\left[x_{n i} \log \mu_{k i}+\left(1-x_{n i}\right) \log \left(1-\mu_{k i}\right)\right]\right]
$$

The code is:
```{r, eval=FALSE}
  #Log likelihood computation.
  for(i in 1:n ){
    for(j in 1:k){
      llik[it] <- llik[it] + z[i,j] * (log(pi[j]) + sum(x[i,] * log(mu[j,]) + (1- x[i,])*log(1- mu[j,]))) 
    }
  } 
 
  
```





ML parameter estimation from the data and fractional component assignments we use:

$$
\begin{aligned}
\pi_{k}^{M L} &=\frac{\sum_{n} p\left(z_{n k} \mid \boldsymbol{x}_{n}, \boldsymbol{\mu}, \boldsymbol{\pi}\right)}{N} \\
\mu_{k i}^{M L} &=\frac{\sum_{n} x_{n i} p\left(z_{n k} \mid \boldsymbol{x}_{n}, \boldsymbol{\mu}, \boldsymbol{\pi}\right)}{\sum_{n} p\left(z_{n k} \mid \boldsymbol{x}_{n}, \boldsymbol{\mu}, \boldsymbol{\pi}\right)}
\end{aligned}
$$
The code is:

```{r, eval=FALSE}
  #M-step: ML parameter estimation from the data and fractional component assignments
  row_sum_z <- c(rep(NA, ncol(z)))
  for (i in 1:ncol(z)) {
    row_sum_z[i] <- sum(z[,i])
  }
```


```{r}
EM_algo(2)
```
When K is equal to 2 the EM-algorithm stops after 16 iterations and the pi values are very close to each other. In this case we miss one true pi, which maybe can be seen as under fitting.

```{r}
EM_algo(3)
```
When K is equal to 3 the EM-algorithm stops after 62 iterations and the pi values are pretty close to each other.

```{r}
EM_algo(4)
```

When K is equal to 4 the EM-algorithm stops after 16 iterations and the pi values are pretty far from each other. In this task we have one extra pi, which maybe can be seen as over fitting.

# Assignment 3


# Appendix:

```{r ref.label = knitr::all_labels(), echo=TRUE, eval=FALSE}
```